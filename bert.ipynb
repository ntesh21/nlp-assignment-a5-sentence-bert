{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The \"simple-wiki\" dataset comprises pairs of equivalent sentences sourced from Wikipedia. Each example in the dataset is structured as a dictionary with the key \"set\" and a list containing two sentences as the corresponding \"value\".\n",
    "\n",
    "Supported Tasks:\n",
    "\n",
    "Sentence Transformers Training: The dataset is specifically designed for training Sentence Transformers models. These models are valuable for tasks related to semantic search and sentence similarity.\n",
    "Languages:\n",
    "\n",
    "The dataset is available in English.\n",
    "\n",
    "Source: https://huggingface.co/datasets/embedding-data/simple-wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code reads data from a Parquet file located at the specified path using Pandas. The resulting data is stored in a DataFrame, enabling further exploration, analysis, and manipulation of the data using Pandas functionalities. Ensure that the necessary libraries are installed (pandas and pyarrow) before running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "parquet_file = \"data/0000.parquet\"\n",
    "df = pd.read_parquet(parquet_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for idx,rows in df.iterrows():\n",
    "    if idx > 20:\n",
    "        break\n",
    "    for sent in rows['set']:\n",
    "        # print(sent)\n",
    "        text += sent+'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code aggregates sentences from the 'set' column of the first 21 rows of the DataFrame df into a single string (text). The loop breaks after processing 21 rows. This type of operation might be useful for creating a consolidated text summary or for further text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp.max_length = 3030000 # or higher\n",
    "doc = nlp(text)\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "#lower case, and clean all the symbols\n",
    "text = [x.text.lower() for x in sentences]\n",
    "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizes the text into sentences, converts them to lowercase, and removes specified punctuation symbols. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making vocabs - numericalization\n",
    "word_list = list(set(\" \".join(text).split()))\n",
    "word2id   = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a vocabulary and corresponding word-to-id mapping for a list of cleaned sentences stored in the variable text. The vocabulary includes unique words in the text, and some special tokens like '[PAD]', '[CLS]', '[SEP]', and '[MASK]' are assigned specific identifiers in the word-to-id mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(word_list):\n",
    "    word2id[w] = i + 4 #reserve the first 0-3 for CLS, PAD\n",
    "    id2word    = {i:w for i, w  in enumerate(word2id)}\n",
    "    vocab_size = len(word2id)\n",
    "    \n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word2id[word] for sentence in text for word in sentence.split()]\n",
    "    token_list.append(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extends the vocabulary and word-to-id mapping created earlier and then uses this mapping to convert sentences into tokenized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "max_mask   = 5 #even though it does not reach 15% yet....maybe you can set this threshold\n",
    "max_len    = 2000 #maximum length that my transformer will accept.....all sentence will be padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0\n",
    "    while positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        \n",
    "        #randomly choose two sentence\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences))\n",
    "        tokens_a, tokens_b            = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        \n",
    "        #1. token embedding - add CLS and SEP\n",
    "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
    "        \n",
    "        #2. segment embedding - which sentence is 0 and 1\n",
    "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "        #3 masking\n",
    "        n_pred = min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
    "        #get all the pos excluding CLS and SEP\n",
    "        candidates_masked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] \n",
    "                                 and token != word2id['[SEP]']]\n",
    "        shuffle(candidates_masked_pos)\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        #simply loop and mask accordingly\n",
    "        for pos in candidates_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_ids[pos])\n",
    "            if random() < 0.1:  #10% replace with random token\n",
    "                index = randint(0, vocab_size - 1)\n",
    "                input_ids[pos] = word2id[id2word[index]]\n",
    "            elif random() < 0.8:  #80 replace with [MASK]\n",
    "                input_ids[pos] = word2id['[MASK]']\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "        #4. pad the sentence to the max length\n",
    "        n_pad = max_len - len(input_ids)\n",
    "        input_ids.extend([0] * n_pad)\n",
    "        segment_ids.extend([0] * n_pad)\n",
    "        \n",
    "        #5. pad the mask tokens to the max length\n",
    "        if max_mask > n_pred:\n",
    "            n_pad = max_mask - n_pred\n",
    "            masked_tokens.extend([0] * n_pad)\n",
    "            masked_pos.extend([0] * n_pad)\n",
    "        \n",
    "        #6. check whether is positive or negative\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "            negative += 1\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates batches of training data that can predict masked tokens within a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 2000]),\n",
       " torch.Size([6, 2000]),\n",
       " torch.Size([6, 5]),\n",
       " torch.Size([6, 5]),\n",
       " tensor([0, 0, 0, 1, 1, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[313, 221,  35, 278,  56],\n",
       "        [138, 234, 143, 253,   5],\n",
       "        [148, 155,  73, 154, 140],\n",
       "        [168, 217, 240, 251, 323],\n",
       "        [261, 141,  85,  47,  19],\n",
       "        [168,  50, 163, 234, 110]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6    # number of Encoder of Encoder Layer\n",
    "n_heads  = 8    # number of heads in Multi-Head Attention\n",
    "d_model  = 768  # Embedding Size\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        #x, seg: (bs, len)\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding class, which is designed to handle token, position, and segment embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_attn_pad_mask** generates an attention mask to handle padding in a sequence. This mask is intended for use in Bert model to prevent attention from being applied to padding tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2000, 2000])\n"
     ]
    }
   ],
   "source": [
    "print(get_attn_pad_mask(input_ids, input_ids).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ScaledDotProductAttention** represents the scaled dot-product attention mechanism commonly used in Bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MultiHeadAttention** represents the multi-head attention mechanism in a Bert model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
    "        return self.fc2(F.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PoswiseFeedForwardNet** represents the position-wise feedforward network within a Bert model. This network consists of two linear layers with a GELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
    "        return enc_outputs, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EncoderLayer**  represents one layer of the encoder in a Bert model. This layer consists of a multi-head self-attention mechanism (MultiHeadAttention) followed by a position-wise feedforward network (PoswiseFeedForwardNet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
    "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
    "        \n",
    "        # 1. predict next sentence\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
    "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
    "\n",
    "        # 2. predict the masked token\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
    "\n",
    "        return logits_lm, logits_nsp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch imlementation of Bert\n",
    "\n",
    "Embedding Layer:\n",
    "\n",
    "The model starts with an embedding layer (Embedding), responsible for token, position, and segment embeddings.\n",
    "Encoder Layers:\n",
    "\n",
    "The model consists of multiple encoder layers (EncoderLayer) organized as a stack (nn.ModuleList). Each encoder layer includes a multi-head self-attention mechanism and a position-wise feedforward network, which are crucial components of the Transformer architecture.\n",
    "Additional Linear and Activation Layers:\n",
    "\n",
    "After the encoder layers, the model includes additional linear and activation layers for downstream tasks:\n",
    "fc: A linear layer followed by a hyperbolic tangent (Tanh) activation function.\n",
    "linear: Another linear layer.\n",
    "norm: A layer normalization operation.\n",
    "classifier: A linear layer for predicting the next sentence in a binary classification task.\n",
    "Decoder Layer for Masked Language Model (MLM):\n",
    "\n",
    "The decoder for the masked language model (MLM) task is shared with the embedding layer.\n",
    "The weights of the decoder are tied to the token embedding weights.\n",
    "A linear layer (decoder) followed by a bias term is used to predict the masked tokens in the input sequence.\n",
    "Forward Method:\n",
    "\n",
    "The forward method takes input_ids (token indices), segment_ids (segment indices), and masked_pos (positions of masked tokens) as inputs.\n",
    "The input undergoes embedding, followed by multiple encoder layers.\n",
    "The encoder self-attention mask is generated to handle padding tokens.\n",
    "The output from the encoder layers is used for two tasks:\n",
    "Predicting the next sentence (logits_nsp) based on the first token's representation.\n",
    "Predicting masked tokens (logits_lm) by attending to the masked positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PREDATOR\\Music\\ML_project\\nlp\\nlp-assignment-a3\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 loss = 84.624443\n",
      "Epoch: 100 loss = 4.239068\n",
      "Epoch: 200 loss = 3.811649\n",
      "Epoch: 300 loss = 3.803038\n",
      "Epoch: 400 loss = 3.716961\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 500\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
    "    #logits_lm: (bs, max_mask, vocab_size) ==> (6, 5, 34)\n",
    "    #logits_nsp: (bs, yes/no) ==> (6, 2)\n",
    "\n",
    "    #1. mlm loss\n",
    "    #logits_lm.transpose: (bs, vocab_size, max_mask) vs. masked_tokens: (bs, max_mask)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    #2. nsp loss\n",
    "    #logits_nsp: (bs, 2) vs. isNext: (bs, )\n",
    "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
    "    \n",
    "    #3. combine loss\n",
    "    loss = loss_lm + loss_nsp\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End-to-end training process for a BERT model on a above dataset. The model is simultaneously trained for both masked language modeling and next sentence prediction tasks, with the overall loss being a combination of both. The synthetic dataset is generated using the make_batch() function, and the training loop updates the model parameters to minimize the combined loss over the specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "torch.save(model.state_dict(), 'bert_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "pretrained_model = model.load_state_dict(torch.load('bert_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'greatest', 'example', 'has', 'been', 'in', 'his', 'present', 'job', '(', 'then', 'minister', 'for', 'foreign', 'affairs', ')', 'where', 'he', 'has', 'perforce', 'concentrated', 'on', 'angloirish', 'relations', 'and', 'in', 'particular', 'northern', 'ireland', '(', 'the', 'greatest', 'example', 'has', 'been', 'in', 'his', 'present', 'job', '[MASK]', 'then', 'minister', 'for', 'foreign', 'affairs', ')', 'where', 'he', 'has', 'perforce', 'concentrated', 'on', 'angloirish', 'relations', 'and', 'in', 'particular', 'the', 'north', '(', 'ie', 'northern', 'ireland', ')', 'president', 'hillery', 'refused', 'to', 'speak', 'to', 'any', 'opposition', 'party', 'politicians', 'but', 'when', 'charles', 'haughey', 'who', 'was', 'leader', 'of', 'the', 'opposition', 'had', 'rang', 'the', 'president', \"'s\", 'office', 'he', 'threatened', 'to', 'end', 'the', 'career', 'of', 'the', 'army', 'officer', 'answered', 'and', 'refused', 'on', 'hillery', \"'s\", 'explicit', 'orders', 'to', 'put', 'the', 'call', 'through', 'to', 'the', 'president', 'his', 'reputation', 'rose', 'further', 'when', 'opposition', 'leaders', 'under', 'parliamentary', 'privilege', 'alleged', 'that', 'taoiseach', 'charles', 'haughey', 'who', 'in', 'january', '1982', 'had', 'been', 'leader', 'of', 'the', 'opposition', 'had', 'not', 'merely', 'rung', 'the', 'president', \"'s\", 'office', 'but', 'threatened', 'to', 'end', 'the', 'career', 'of', 'the', 'army', 'officer', 'who', 'took', 'the', 'call', 'and', 'who', 'on', 'hillery', \"'s\", 'explicit', 'instructions', 'had', 'refused', 'to', 'put', 'through', 'the', 'call', 'to', 'the', 'president', 'he', 'thought', 'about', 'returning', 'to', 'medicine', 'perhaps', 'moving', 'with', 'his', 'wife', 'maeve', '(', 'also', 'a', 'doctor', ')', 'to', 'africa', 'he', 'considered', 'returning', 'to', 'medicine', 'perhaps', 'moving', 'with', 'his', 'wife', 'maeve', '(', 'also', 'a', 'doctor', ')', 'to', 'africa', 'hillery', \"'s\", 'most', 'famous', 'policy', 'was', 'to', 'force', 'eec', 'member', 'states', 'to', 'give', 'equal', 'pay', 'to', 'women', 'as', 'social', 'affairs', 'commissioner', 'hillery', \"'s\", 'most', 'famous', 'policy', 'initiative', 'was', 'to', 'force', 'eec', 'member', 'states', 'to', 'give', 'equal', 'pay', 'to', 'women', 'when', 'president', 'cearbhall', 'ã', '``', 'dã', 'laigh', 'resigned', 'hillery', 'agreed', 'to', 'become', 'the', 'fianna', 'fã', 'il', 'candidate', 'in', 'the', 'election', 'when', 'a', 'furious', 'president', 'ã', '``', 'dã', 'laigh', 'resigned', 'a', 'deeply', 'reluctant', 'hillery', 'agreed', 'to', 'become', 'the', 'fianna', 'fã', 'il', 'candidate', 'for', 'the', 'presidency', 'won', 'the', 'election', 'and', 'hillery', 'was', 'successful', 'on', 'his', 'first', 'attempt', 'to', 'get', 'elected', 'the', 'election', 'resulted', 'in', 'a', 'return', 'to', 'power', 'for', 'fianna', 'fã', 'il', 'and', 'hillery', 'was', 'successful', 'on', 'his', 'first', 'attempt', 'to', 'get', 'elected', 'dr', 'patrick', 'john', 'hillery', '(', ';', '2', 'may', '1923', 'april', '12', '2008', ')', 'is', 'an', 'irish', 'fianna', 'fã', 'il', 'politician', 'and', 'the', 'sixth', 'president', 'of', 'ireland', 'from', '1976', 'until', '1990', 'patrick', 'john', '``', 'paddy', \"''\", 'hillery', '(', '2', 'may', '1923', 'â', '``', '12', 'april', '2008', ')', 'was', 'an', 'irish', 'fianna', 'fã', 'il', 'politician', 'and', 'the', 'sixth', 'president', 'of', 'ireland', 'from', '1976', 'until', '1990', 'in', '1947', 'he', 'returned', 'to', 'his', 'home', 'town', 'where', 'he', 'followed', 'in', 'his', 'fatherâ', 's', 'footsteps', 'as', 'a', 'doctor', 'upon', 'his', 'conferral', 'in', '1947', 'he', 'returned', 'to', 'his', 'native', 'town', 'where', 'he', 'followed', 'in', 'his', 'fatherâ', 's', 'footsteps', 'as', 'a', 'doctor', 'he', 'attended', 'university', 'college', 'dublin', 'where', 'he', 'studied', 'medicine', 'at', 'third', 'level', 'hillery', 'attended', 'university', 'college', 'dublin', 'where', 'he', 'qualified', 'with', 'a', 'degree', 'in', 'medicine', 'hillery', 'married', 'mary', 'beatrice', '(', 'maeve', ')', 'finnegan', 'on', 'october', '27', '1955', 'hillery', 'married', 'mary', 'beatrice', '(', 'maeve', ')', 'finnegan', 'on', '27', 'october', '1955', 'policy', 'in', 'this', 'field', 'is', 'determined', 'primarily', 'between', 'him', 'and', 'the', '(', '(', 'taoiseach', ';', 'and', 'it', 'is', 'likely', 'that', 'the', 'fianna', 'fã', 'il', 'new', 'line', 'owes', 'much', 'to', 'dr', 'hillery', 'policy', 'in', 'this', 'field', 'is', 'determined', 'primarily', 'between', 'him', 'and', 'the', 'taoiseach', ';', 'and', 'it', 'is', 'likely', 'that', 'the', 'fianna', 'fã', 'il', 'new', 'line', 'owes', 'much', 'to', 'dr', 'hillery', 'first', 'elected', 'at', 'the', '1951', 'general', 'election', 'as', 'a', 'fianna', 'fã', 'il', 'td', 'for', 'county', 'clare', 'he', 'remained', 'in', 'dã', 'il', 'ã', 'ireann', 'until', '1973', 'first', 'elected', 'at', 'the', '1951', 'general', 'election', 'as', 'a', 'fianna', 'fã', 'il', 'teachta', 'dã', 'la', '(', 'td', ')', 'for', 'clare', 'he', 'remained', 'in', 'dã', 'il', 'ã', 'ireann', 'until', '1973', 'a', 'church', 'was', 'founded', 'in', '860', 'by', 'ludger', 'liudger', 'as', 'well', 'as', 'the', 'first', 'monastery', 'in', 'westphalia', 'a', 'church', 'was', 'founded', 'in', '860', 'by', 'liudger', 'as', 'well', 'as', 'the', 'first', 'convent', 'in', 'westphalia', 'nottuln', 'is', 'a', 'village', '20', 'km', 'of', 'mã', '1\\\\/4', 'nster', 'germany', '20', 'km', 'west', 'of', 'mã', '1\\\\/4', 'nster', 'in', '1939', 'the', 'orchestra', \"'s\", 'sponsors', 'stopped', 'giving', 'money', 'to', 'the', 'orchestra', 'the', 'orchestra', 'became', 'a', 'selfgoverning', 'body', 'which', 'meant', 'that', 'like', 'the', 'london', 'symphony', 'orchestra', 'the', 'players', 'organized', 'the', 'orchestra', 'themselves', 'deciding', 'on', 'their', 'pay', 'their', 'members', 'what', 'music', 'they', 'should', 'play', 'etc', 'in', '1939', 'the', 'orchestra', \"'s\", 'sponsors', 'withdrew', 'their', 'financial', 'support', 'and', 'the', 'orchestra', 'became', 'selfgoverning', 'with', 'members', 'of', 'the', 'orchestra', 'themselves', 'taking', 'decisions', 'on', 'the', 'organization', \"'s\", 'affairs', 'in', 'the', '1930s', 'when', 'beecham', 'conducted', 'at', 'the', 'royal', 'opera', 'house', 'the', 'orchestra', 'played', 'for', 'operas', 'there', 'in', 'the', '1930s', 'the', 'lpo', 'was', 'the', 'orchestra', 'for', 'the', 'international', 'opera', 'seasons', 'at', 'the', 'royal', 'opera', 'house', 'covent', 'garden', 'of', 'which', 'beecham', 'was', 'artistic', 'director', 'in', '1964', 'the', 'lpo', 'became', 'the', 'resident', 'orchestra', 'for', 'glyndebourne', 'festival', 'opera', 'in', 'the', 'summer', 'in', 'addition', 'the', 'lpo', 'is', 'the', 'main', 'resident', 'orchestra', 'of', 'the', 'glyndebourne', 'festival', 'opera', 'the', 'london', 'philharmonic', 'orchestra', '(', 'lpo', ')', 'is', 'one', 'of', 'the', 'major', 'orchestras', 'of', 'the', 'united', 'kingdom', 'it', 'is', 'based', 'in', 'the', 'royal', 'festival', 'hall', 'london', 'the', 'london', 'philharmonic', 'orchestra', '(', 'lpo', ')', 'based', 'in', 'london', 'is', 'one', 'of', 'the', 'major', 'orchestras', 'of', 'the', 'united', 'kingdom', 'and', 'is', 'based', 'in', 'the', 'royal', 'festival', 'hall', 'at', 'one', 'of', 'the', 'orchestra', \"'s\", 'early', 'concerts', 'in', 'november', '1932', 'the', 'sixteenyear', 'old', 'yehudi', 'menuhin', 'played', 'a', 'program', 'of', 'violin', 'concertos', 'including', 'the', 'concerto', 'by', 'elgar', 'which', 'the', 'composer', 'himself', 'conducted', 'at', 'one', 'of', '[MASK]', 'orchestra', \"'s\", 'early', 'concerts', 'in', 'november', '1932', 'the', 'sixteenyear', 'old', 'yehudi', 'menuhin', 'played', 'a', 'program', 'of', 'violin', 'concertos', ';', 'those', 'by', 'bach', 'and', 'mozart', 'were', 'conducted', 'by', 'beecham', 'and', 'elgar', \"'s\", 'concerto', 'in', 'b', 'minor', 'was', 'conducted', 'by', 'the', 'composer', 'the', 'associate', 'conductor', 'at', 'the', 'time', 'was', 'malcolm', 'sargent', 'its', 'founding', 'associate', 'conductor', 'was', 'malcolm', 'sargent', 'the', 'orchestra', 'was', 'formed', 'in', '1932', 'by', 'sir', 'thomas', 'beecham', 'and', 'played', 'its', 'first', 'concert', 'on', 'october', '7', '1932', 'at', 'the', 'queen', \"'s\", 'hall', 'london', 'the', 'orchestra', 'was', 'formed', 'in', '1932', 'by', 'sir', 'thomas', 'beecham', 'and', 'played', 'its', 'first', 'concert', 'on', '7', 'october', '1932', 'at', 'the', 'queen', \"'s\", 'hall', 'london', '[SEP]', 'the', 'greatest', 'example', 'has', 'been', 'in', 'his', 'present', 'job', '(', 'then', 'minister', 'for', 'foreign', 'affairs', ')', 'where', 'he', 'has', 'perforce', 'concentrated', 'on', 'angloirish', 'relations', 'and', 'in', 'particular', 'northern', 'ireland', '(', 'the', 'greatest', 'example', 'has', 'been', 'in', 'his', 'present', 'job', '(', 'then', 'minister', 'for', 'foreign', 'affairs', ')', 'where', 'he', 'has', 'perforce', 'concentrated', 'on', 'angloirish', 'relations', 'and', 'in', 'particular', 'the', 'north', '(', 'ie', 'northern', 'ireland', ')', 'president', 'hillery', 'refused', 'to', 'speak', 'to', 'any', 'opposition', 'party', 'politicians', 'but', 'when', 'charles', 'haughey', 'who', 'was', 'leader', 'of', 'the', 'opposition', 'had', 'rang', 'the', 'president', \"'s\", 'office', 'he', 'threatened', 'to', 'end', 'the', 'career', 'of', 'the', 'army', 'officer', 'answered', 'and', 'refused', 'on', 'hillery', \"'s\", 'explicit', 'orders', 'to', 'put', 'the', 'call', 'through', 'to', 'the', 'president', 'his', 'reputation', 'rose', 'further', 'when', 'opposition', 'leaders', 'under', 'parliamentary', 'privilege', 'alleged', 'that', 'taoiseach', 'charles', 'haughey', 'who', 'in', 'january', 'parliamentary', 'had', 'been', 'leader', 'of', 'the', 'opposition', 'had', 'not', 'merely', 'rung', 'the', 'president', \"'s\", 'office', 'but', 'threatened', 'to', 'end', 'the', 'career', 'of', 'the', 'army', 'officer', 'who', 'took', 'college', 'call', 'and', 'who', 'on', 'hillery', \"'s\", 'explicit', 'instructions', 'had', 'refused', 'to', 'put', 'through', 'the', 'call', 'to', 'the', 'president', 'he', 'thought', 'about', 'returning', 'to', 'medicine', 'perhaps', 'moving', 'with', 'his', 'wife', 'maeve', '(', 'also', 'a', 'doctor', ')', 'to', 'africa', 'he', 'considered', 'returning', 'to', 'medicine', 'perhaps', 'moving', 'with', 'his', 'wife', 'maeve', '(', 'also', 'a', 'doctor', ')', 'to', 'africa', 'hillery', \"'s\", 'most', 'famous', 'policy', 'was', 'to', 'force', 'eec', 'member', 'states', 'to', 'give', 'equal', 'pay', 'to', 'women', 'as', 'social', 'affairs', 'commissioner', 'hillery', \"'s\", 'most', 'famous', 'policy', 'initiative', 'was', 'to', 'force', 'eec', 'member', 'states', 'to', 'give', '[MASK]', 'pay', 'to', 'women', 'when', 'president', 'cearbhall', 'ã', '``', 'dã', 'laigh', 'resigned', 'hillery', 'agreed', 'to', 'become', 'the', 'fianna', 'fã', 'il', 'candidate', 'in', 'the', 'election', 'when', 'a', 'furious', 'president', 'ã', '``', 'dã', 'laigh', 'resigned', 'a', 'deeply', 'reluctant', 'hillery', 'agreed', 'to', 'become', 'the', 'fianna', 'fã', 'il', 'candidate', 'for', 'the', 'presidency', 'won', 'the', 'election', 'and', 'hillery', 'was', 'successful', 'on', 'his', 'first', 'attempt', 'to', 'get', 'elected', 'the', 'election', 'resulted', 'in', 'a', 'return', 'to', 'power', 'for', 'fianna', 'fã', 'il', 'and', 'hillery', 'was', 'successful', 'on', 'his', 'first', 'attempt', 'to', 'get', 'elected', 'dr', 'patrick', 'john', 'hillery', '(', ';', '2', 'may', '1923', 'april', '12', '2008', ')', 'is', 'an', 'irish', 'fianna', 'fã', 'il', 'politician', 'and', 'the', 'sixth', 'president', 'of', 'ireland', 'from', '1976', 'until', '1990', 'patrick', 'john', '``', 'paddy', \"''\", 'hillery', '(', '2', 'may', '1923', 'â', '``', '12', 'april', '2008', ')', 'was', 'an', 'irish', 'fianna', 'fã', 'il', 'politician', 'and', 'the', 'sixth', 'president', 'of', 'ireland', 'from', '1976', 'until', '1990', 'in', '1947', 'he', 'returned', 'to', 'his', 'home', 'town', 'where', 'he', 'followed', 'in', 'his', 'fatherâ', 's', 'footsteps', 'as', 'a', 'doctor', 'upon', 'his', 'conferral', 'in', '1947', 'he', 'returned', 'to', 'his', 'native', 'town', 'where', 'he', 'followed', 'in', 'his', 'fatherâ', 's', 'footsteps', 'as', 'a', 'doctor', 'he', 'attended', 'university', 'college', 'dublin', 'where', 'he', 'studied', 'medicine', 'at', 'third', 'level', 'hillery', 'attended', 'university', 'college', 'dublin', 'where', 'he', 'qualified', 'with', 'a', 'degree', 'in', 'medicine', 'hillery', 'married', 'mary', 'beatrice', '(', 'maeve', ')', 'finnegan', 'on', 'october', '27', '1955', 'hillery', 'married', 'mary', 'beatrice', '(', 'maeve', ')', 'finnegan', 'on', '27', 'october', '1955', 'policy', 'in', 'this', 'field', 'is', 'determined', 'primarily', 'between', 'him', 'and', 'the', '(', '(', 'taoiseach', ';', 'and', 'it', 'is', 'likely', 'that', 'the', 'fianna', 'fã', 'il', 'new', 'line', 'owes', 'much', 'to', 'dr', 'hillery', 'policy', 'in', 'this', 'field', 'is', 'determined', 'primarily', 'between', 'him', 'and', 'the', 'taoiseach', ';', 'and', 'it', 'is', 'likely', 'that', 'the', 'fianna', 'fã', 'il', 'new', 'line', 'owes', 'much', 'to', 'dr', 'hillery', 'first', 'elected', 'at', 'the', '1951', 'general', 'election', 'as', 'a', 'fianna', 'fã', 'il', 'td', 'for', 'county', 'clare', 'he', 'remained', 'in', 'dã', 'il', 'ã', 'ireann', 'until', '1973', 'first', 'elected', 'at', 'the', '1951', 'general', 'election', 'as', 'a', 'fianna', 'fã', 'il', 'teachta', 'dã', 'la', '(', 'td', ')', 'for', 'clare', 'he', 'remained', 'in', 'dã', 'il', 'ã', 'ireann', 'until', '1973', 'a', 'church', 'was', 'founded', 'in', '860', 'by', 'ludger', 'liudger', 'as', 'well', 'as', 'the', 'first', 'monastery', 'in', 'westphalia', 'a', 'church', 'was', 'founded', 'in', '860', 'by', 'liudger', 'as', 'well', 'as', 'the', 'first', 'convent', 'in', 'westphalia', 'nottuln', 'is', 'a', 'village', '20', 'km', 'of', 'mã', '1\\\\/4', 'nster', 'germany', '20', 'km', 'west', 'of', 'mã', '1\\\\/4', 'nster', 'in', '1939', 'the', 'orchestra', \"'s\", 'sponsors', 'stopped', 'giving', 'money', 'to', 'the', 'orchestra', 'the', 'orchestra', 'became', 'a', 'selfgoverning', 'body', 'which', 'meant', 'that', 'like', 'the', 'london', 'symphony', 'orchestra', 'the', 'players', 'organized', 'the', 'orchestra', 'themselves', 'deciding', 'on', 'their', 'pay', 'their', 'members', 'what', 'music', 'they', 'should', 'play', 'etc', 'in', '1939', 'the', 'orchestra', \"'s\", 'sponsors', 'withdrew', 'their', 'financial', 'support', 'and', 'the', 'orchestra', 'became', 'selfgoverning', 'with', 'members', 'of', 'the', 'orchestra', 'themselves', 'taking', 'decisions', 'on', 'the', 'organization', \"'s\", 'affairs', 'in', 'the', '1930s', 'when', 'beecham', 'conducted', 'at', 'the', 'royal', 'opera', 'house', 'the', 'orchestra', 'played', 'for', 'operas', 'there', 'in', 'the', '1930s', 'the', 'lpo', 'was', 'the', 'orchestra', 'for', 'the', 'international', 'opera', 'seasons', 'at', 'the', 'royal', 'opera', 'house', 'covent', 'garden', 'of', 'which', 'beecham', 'was', 'artistic', 'director', 'in', '1964', 'the', 'lpo', 'became', 'the', 'resident', 'orchestra', 'for', 'glyndebourne', 'festival', 'opera', 'in', 'the', 'summer', 'in', 'addition', 'the', 'lpo', 'is', 'the', 'main', 'resident', 'orchestra', 'of', 'the', 'glyndebourne', 'festival', 'opera', 'the', 'london', 'philharmonic', 'orchestra', '(', 'lpo', ')', 'is', 'one', 'of', 'the', 'major', 'orchestras', 'of', 'the', 'united', 'kingdom', 'it', 'is', 'based', 'in', 'the', 'royal', 'festival', 'hall', 'london', 'the', 'london', 'philharmonic', 'orchestra', '(', 'lpo', ')', 'based', 'in', 'london', 'is', 'one', 'of', 'the', 'major', 'orchestras', 'of', 'the', 'united', 'kingdom', 'and', 'is', 'based', 'in', 'the', 'royal', 'festival', 'hall', 'at', 'one', 'of', 'the', 'orchestra', \"'s\", 'early', 'concerts', 'in', 'november', '1932', 'the', 'sixteenyear', 'old', 'yehudi', 'menuhin', 'played', 'a', 'program', 'of', 'violin', 'concertos', 'including', 'the', 'concerto', 'by', 'elgar', 'which', 'the', 'composer', 'himself', 'conducted', 'at', 'one', 'of', 'the', 'orchestra', \"'s\", 'early', 'concerts', 'in', 'november', '1932', 'the', 'sixteenyear', 'old', 'yehudi', 'menuhin', 'played', 'a', 'program', 'of', 'violin', 'concertos', ';', 'those', 'by', 'bach', 'and', 'mozart', 'were', 'conducted', 'by', 'beecham', 'and', 'elgar', \"'s\", 'concerto', 'in', 'b', 'minor', 'was', 'conducted', 'by', 'the', 'composer', 'the', 'associate', 'conductor', 'at', 'the', 'time', 'was', 'malcolm', 'sargent', 'its', 'founding', 'associate', 'conductor', 'was', 'malcolm', 'sargent', 'the', 'orchestra', 'was', 'formed', 'in', '1932', 'by', 'sir', 'thomas', 'beecham', 'and', 'played', 'its', 'first', 'concert', 'on', 'october', '7', '1932', 'at', 'the', 'queen', \"'s\", 'hall', 'london', 'the', 'orchestra', 'was', 'formed', 'in', '1932', 'by', 'sir', 'thomas', 'beecham', 'and', 'played', 'its', 'first', 'concert', 'on', '7', 'october', '1932', 'at', 'the', 'queen', \"'s\", 'hall', 'london', '[SEP]']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_IncompatibleKeys' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[147], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m input_ids, segment_ids, masked_tokens, masked_pos, isNext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(torch\u001b[38;5;241m.\u001b[39mLongTensor, \u001b[38;5;28mzip\u001b[39m(batch[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m([id2word[w\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m input_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m id2word[w\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m logits_lm, logits_nsp \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#logits_nsp: (1, yes/no) ==> (1, 2)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#predict masked tokens\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\u001b[39;00m\n\u001b[0;32m     11\u001b[0m logits_lm \u001b[38;5;241m=\u001b[39m logits_lm\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy() \n",
      "\u001b[1;31mTypeError\u001b[0m: '_IncompatibleKeys' object is not callable"
     ]
    }
   ],
   "source": [
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
    "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_nsp = pretrained_model(input_ids, segment_ids, masked_pos)\n",
    "#logits_lm:  (1, max_mask, vocab_size) ==> (1, 5, 34)\n",
    "#logits_nsp: (1, yes/no) ==> (1, 2)\n",
    "\n",
    "#predict masked tokens\n",
    "#max the probability along the vocab dim (2), [1] is the indices of the max, and [0] is the first value\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy() \n",
    "#note that zero is padding we add to the masked_tokens\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
    "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
    "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
    "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
    "\n",
    "#predict nsp\n",
    "logits_nsp = logits_nsp.data.max(1)[1][0].data.numpy()\n",
    "print(logits_nsp)\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_nsp else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 234, 175,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task 2. Sentence Embedding with Sentence BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SNLI dataset\n",
    "dataset = load_dataset('snli', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(examples):\n",
    "    # Tokenize the sentence pairs\n",
    "    # tokenized_data = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    return pretrained_tokenizer(examples['premise'], return_tensors='pt', padding=True, truncation=True)\n",
    "    # return pretrained_model(examples['premise'], examples['hypothesis'], padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = dataset.map(encode_examples, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and BERT model\n",
    "# tokenizer = model.from_pretrained('bert_model')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(examples):\n",
    "    # Tokenize the sentence pairs\n",
    "    # tokenized_data = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "    return pretrained_tokenizer(examples['premise'], return_tensors='pt', padding=True, truncation=True)\n",
    "    # return pretrained_model(examples['premise'], examples['hypothesis'], padding='max_length', truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:04<00:00, 2333.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode the SNLI dataset\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenized_data = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "encoded_dataset = dataset.map(encode_examples, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, bert_model, fc_dim=768):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(fc_dim, 1)\n",
    "\n",
    "    def forward(self, premise_input_ids, premise_token_type_ids, premise_attention_mask,\n",
    "                hypothesis_input_ids, hypothesis_token_type_ids, hypothesis_attention_mask):\n",
    "        # Forward pass for premise\n",
    "        _, premise_embedding = self.bert(\n",
    "            input_ids=premise_input_ids,\n",
    "            token_type_ids=premise_token_type_ids,\n",
    "            attention_mask=premise_attention_mask\n",
    "        )\n",
    "\n",
    "        # Forward pass for hypothesis\n",
    "        _, hypothesis_embedding = self.bert(\n",
    "            input_ids=hypothesis_input_ids,\n",
    "            token_type_ids=hypothesis_token_type_ids,\n",
    "            attention_mask=hypothesis_attention_mask\n",
    "        )\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        cosine_similarity = self.compute_cosine_similarity(premise_embedding, hypothesis_embedding)\n",
    "\n",
    "        # Apply fully connected layer\n",
    "        output = self.fc(cosine_similarity)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_cosine_similarity(self, x1, x2):\n",
    "        # Compute cosine similarity between two vectors\n",
    "        dot_product = torch.sum(x1 * x2, dim=-1)\n",
    "        norm_x1 = torch.norm(x1, dim=-1)\n",
    "        norm_x2 = torch.norm(x2, dim=-1)\n",
    "        cosine_similarity = dot_product / (norm_x1 * norm_x2 + 1e-8)  # Add epsilon to avoid division by zero\n",
    "        return cosine_similarity\n",
    "\n",
    "    def classification_loss(self, logits, label):\n",
    "        # Classification objective function with softmax\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        probabilities = softmax(logits)\n",
    "        loss = nn.CrossEntropyLoss()(probabilities, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(encoded_dataset, batch_size=batch_size)\n",
    "siamese_model = SiameseNetwork(pretrained_model, fc_dim=768)\n",
    "batch_size = 6\n",
    "dataloader = DataLoader(encoded_dataset, batch_size=batch_size)\n",
    "# Set up the optimizer\n",
    "num_epoch = 500\n",
    "embedding_dim = 768 \n",
    "# Assuming you have data with keys ['premise', 'hypothesis', 'label']\n",
    "# You need to tokenize the input sentences and prepare input tensors\n",
    "premise_text = \"This is a sample premise.\"\n",
    "hypothesis_text = \"This is a sample hypothesis.\"\n",
    "label = torch.tensor([1])  # Example label, can be 0 or 1\n",
    "\n",
    "# Tokenize input sentences\n",
    "premise_tokens = pretrained_tokenizer(premise_text, return_tensors='pt', padding=True, truncation=True)\n",
    "hypothesis_tokens = pretrained_tokenizer(hypothesis_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Extract input tensors\n",
    "premise_input_ids = premise_tokens['input_ids']\n",
    "premise_token_type_ids = premise_tokens['token_type_ids']\n",
    "premise_attention_mask = premise_tokens['attention_mask']\n",
    "\n",
    "hypothesis_input_ids = hypothesis_tokens['input_ids']\n",
    "hypothesis_token_type_ids = hypothesis_tokens['token_type_ids']\n",
    "hypothesis_attention_mask = hypothesis_tokens['attention_mask']\n",
    "\n",
    "# Forward pass through the Siamese network\n",
    "output = siamese_model(\n",
    "    premise_input_ids=premise_input_ids,\n",
    "    premise_token_type_ids=premise_token_type_ids,\n",
    "    premise_attention_mask=premise_attention_mask,\n",
    "    hypothesis_input_ids=hypothesis_input_ids,\n",
    "    hypothesis_token_type_ids=hypothesis_token_type_ids,\n",
    "    hypothesis_attention_mask=hypothesis_attention_mask\n",
    ")\n",
    "\n",
    "# Calculate classification loss\n",
    "loss = siamese_model.classification_loss(output, label)\n",
    "\n",
    "print(\"Siamese Network Output:\", output.item())\n",
    "print(\"Classification Loss:\", loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
