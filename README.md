Bert Training
=========================

The task implements a BERT (Bidirectional Encoder Representations from Transformers) model for two primary tasks: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). BERT is a pre-trained Transformer-based model designed for various natural language understanding tasks.

1. Masked Language Modeling (MLM) Task:
Model Architecture:
The BERT model consists of multiple encoder layers, each containing a multi-head self-attention mechanism and a position-wise feedforward network.
The encoder layers are stacked using nn.ModuleList.
The model includes additional linear and activation layers for downstream tasks: fc, linear, norm, and classifier.
The decoder for the masked language model (MLM) task shares weights with the embedding layer.
Forward Pass:
The forward method takes input_ids (token indices), segment_ids (segment indices), and masked_pos (positions of masked tokens) as inputs.
The input undergoes embedding, followed by multiple encoder layers.
The encoder self-attention mask is generated to handle padding tokens.
The output from the encoder layers is used to predict the next sentence (logits_nsp) based on the first token's representation and predict masked tokens (logits_lm) by attending to the masked positions.
Loss Calculation:
Two losses are calculated for the MLM task:
loss_lm: CrossEntropyLoss between predicted masked tokens (logits_lm) and actual masked tokens (masked_tokens).
loss_nsp: CrossEntropyLoss between predicted next sentence labels (logits_nsp) and actual labels (isNext).
Overall Loss:
The overall loss is the sum of MLM and NSP losses: loss = loss_lm + loss_nsp.
Training Loop:
The model is trained using the Adam optimizer with a learning rate of 0.001 for 500 epochs.
The training loop involves forward pass, loss calculation, backpropagation, and parameter updates.
The training progress is monitored, and the loss is printed every 100 epochs.

2. Dataset and Data Generation:
Dataset:
The code uses a synthetic dataset generated by the make_batch() function.
The dataset is formatted with input sequences, segment indices, masked tokens, masked positions, and labels for the next sentence prediction.
Training Objective:
The model is trained to simultaneously perform two tasks:
Predict masked tokens in the input sequence (MLM task).
Predict whether the next sentence is a continuation of the current one (NSP task).
Conclusion:

In summary, training a BERT model on a simple wiki dataset for a masked language modeling task and a next sentence prediction task. The model architecture includes multiple encoder layers, and the training process involves minimizing the combined loss of MLM and NSP using the Adam optimizer. The use of synthetic data allows for controlled experimentation and showcases the end-to-end training process for a BERT model on these specific natural language processing tasks.


Web Application
======================================
Flask web application for semantic search using a pre-trained sentence transformer model. 



The web app allows users to input a query through an HTML form.
On form submission, the entered query is processed in the backend.
Pre-trained Sentence Transformer Model:

The code uses the "all-MiniLM-L6-v2" pre-trained sentence transformer model.
This model is capable of encoding sentences into numerical representations that capture semantic similarities.
Data Loading and Corpus Creation:

The web app loads a sample dataset from a Parquet file ("0000.parquet") using Pandas.
The function get_corpus() extracts a limited number of sentences from the dataset to form a corpus.
Cosine Similarity Calculation:

A function named get_cosine_similarity(query, corpus) calculates the cosine similarity between the user's query and each sentence in the corpus.
The function uses the sentence transformer model to encode both the query and the corpus.
Web Interface:

The web interface includes an HTML form with an input box for users to enter their query.
Upon submitting the query, the backend calculates the cosine similarity scores between the query and sentences in the corpus.
The most similar sentence and its similarity score are displayed to the user.
Result Presentation:

The result is presented in the same HTML page, indicating the similarity score and the most similar sentence from the corpus.
Running the Web App:
The Flask application is run with app.run(debug=True).
Users can access the web app through a web browser, and the interface allows them to input queries for semantic search.
Conclusion:
In summary, the web application provides a user-friendly interface for semantic search using a pre-trained sentence transformer model. It loads a sample dataset, creates a corpus, and calculates the cosine similarity between user queries and sentences in the corpus. The results are then presented to the user through the web interface. This application showcases the integration of natural language processing capabilities for semantic search within a Flask web framework.

* Webpage
![alt text](./app/static/app-page.png?raw=true)